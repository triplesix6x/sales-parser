# sales-parser

## Оглавление
 - [Общая информация](#общая-информация)
 - [Конфигурация и CI/CD](#конфигурация-и-cicd)
 - [Сборка и Запуск](#сборка-и-запуск)
 - [PostgreSQL + PGAdmin](#postgresql--pgadmin)
 - [FastAPI + SQLAlchemy](#fastapi--sqlalchemy)
 - [Celery + RabbitMQ](#celery--rabbitmq)
 - [Redis](#redis)
 - [ELK](#elk)
 - [Prometheus + Grafana](#prometheus--grafana)
 

## Общая информация
Данный репозиторий содержит проект микросервиса, который ежедневно получает xml файл с данными по продажам за этот день, записывает данные в базу данных и генерирует на основе этих данных аналитический отчет. Также микросервис предоставляет API для управления данными. 

Структурно проект состоит из основного API, Celery beat и worker'a, базы данных postgres, кэша на redis, очереди сообщений на rabbitmq, а также различных иструментов администрирования: pgadmin для базы данных, ElasticSearch + Logstash + Kibana (в дальнейшием ELK) для сбора и анализа логов микросервиса, и Prometheus и Grafana для сбора и визуализации метрик нагрузки на API. Также в приложении реализованы миграции с помощью Alembic. Также проект покрыт тестами.

**Проект и все контейнеры в нем запускается с помощью docker-compose**

## Сборка и Запуск
В данный момент проект автоматически развёртывается на моём личном ПК при помощи Github Action. 

Для ручного запуска необходимо:
```bash
# Склонировать репозиторий
git clone git@github.com:triplesix6x/sales-parser.git

# Сконфигурировать .env при необходимости
# Запустить с помощью docker-compose
docker-compose up --build -d
```

## Конфигурация и CI/CD
Проект конфигурируется с помощью двух файлов:
 - .env.template - шаблонный файл
 - .env - генерируется на этапе сборки проекта

Для того, чтобы настроить ссылки на базу данных, редис, или на скачиваения файла, необходимо отредактировать соответствующие секретные переменные github action, которые будут участвовать в сборке. 

Файл .env.template используется, если по каким-либо причинам .env не был обнаружен. 

В проекте есть модуль config.py, в котором хранятся все основные настройки, реализованные через pydantic-settings для дополнительной валидации данных.
## PostgreSQL + PGAdmin
База данных состоит из двух таблиц: sales и products. В sales хранится информация о дате продаж и аналитический отчёт, в productrs - сама информация о продуктах. Таблицы генерируется через миграции Alembic.

Контейнер PostgreSQL запускается на порту 5432, pgadmin на порту 80, но из контейнера пробрасывается 8080
## FastAPI + SQLAlchemy
Контейнер FastAPI запускается на порту 6099 и перейдя по маршруту /docs можно просмотреть swagger документацию к API.
С помощью Fastapi реализованы следюущие эндпоинты для sales:
 - GET /sales/
 - GET /sales/date/{sale_date}
 - GET /sales/id/{sale_id}
 - GET /sales/report/{sale_date}
 - POST /sales/
 - PATCH /sales/{sale_id}
 - DELETE /sales/{sale_id}

Также, разумеется, к этим эндпоинтам реализованы CRUD функции к базе данных. В данном проекте используется _SQLAlchemy_ для создания запросов к базе данных. 

Стоит отметить что у сервера uvicorn настроено кастомное логирование с помощью файла log_conf.yaml.

## Celery + RabbitMQ
Для получения и парсинга XML файла с продажами, а также генерации отчета LLM используется Celery. Celery beat будет отправлять Celery worker задачи по RabbitMQ. Задача по парсингу файла будет отправляться ежедневно в 12:00 по Московскому времени. Также Celery worker будет отправлять запрос к chatgpt api по генерации аналитического отчёта. Ключ chatgpt api конфигурируется также через .env файл и настраивается через секретные переменные github actions.

Стоит отметить что у Celery настроено кастомное логирование с помощью файла log_conf.yaml.
## Redis
Для кэширования запросов используется No-SQL база данных Redis. Контейнер с Redis поднимается на порту 5379.
## ELK
Стэк ELK необходим для эффективного сбора и анализа логов. Для хранения логов используется ElasticSearch, контейнер с которым поднимается на порту 9200, для передачи логов от приложения к ElasticSearch используется Lohstash, который конфигурируется в logstash.conf и сообщения к которому конфигурируется в log_conf.yaml для всего приложения, поднимается на порту 5000, и Kibana для визуализации логов, поднимается на порту 5601.

В Kibana для просмотра логов необходимо создать Data View с паттерном "service-fastapi-logs-*"
## Prometheus + Grafana
Prometheus и Grafana поднимаются на портах 9090 и 3000 соответственно. В Grafana для авторизации необходимо использовать стандартые логин и пароль admin admin и для визуализации необходимо будет создать новый Dashboard.

